{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yCe21w1TNCIH"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqwK1mkS4iOiPaZptVMmsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusflygar1-hash/AH2179_Project/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the neccesary libraries for the project.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, r2_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "yO-vUlw1yKh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okey-\n",
        "\n",
        "\n",
        "\n",
        "Psuedo Kod.\n",
        "rensa och \"clean\" datan. Ta bort all null values och NaN's.\n",
        "\n",
        "Dela sedan upp i 5 minuters tidsperioder.\n",
        "\n"
      ],
      "metadata": {
        "id": "3hWYm91cq5I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prepping"
      ],
      "metadata": {
        "id": "LHVSQYKfLwaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tds = pd.read_csv('training_dataset.csv', sep=';')\n",
        "ev_ds = pd.read_csv('evaluation_dataset.csv',sep=';')\n",
        "f_ev_ds = pd.read_csv('final_evaluation_dataset.csv', sep=';')\n"
      ],
      "metadata": {
        "id": "-rVQM72Kq4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n training dataset')\n",
        "print(tds.head())\n",
        "print('\\n evaluation dataset------------')\n",
        "print(ev_ds.head())\n",
        "print('\\n final evaluation dataset --------')\n",
        "print(f_ev_ds.head())"
      ],
      "metadata": {
        "id": "ed8Fuwk6KpPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training dataset')\n",
        "print(tds.isna().sum())\n",
        "print('\\n eval dataset')\n",
        "print(ev_ds.isna().sum())\n",
        "print('\\n final eval dataset')\n",
        "print(f_ev_ds.isna().sum())"
      ],
      "metadata": {
        "id": "f4rMw4EXL1_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eH008jxE-G3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop all NaN values.\n",
        "f_ev_ds_noNAN = f_ev_ds.dropna()\n",
        "tds_noNAN = tds.dropna()\n",
        "ev_ds_noNAN = ev_ds.dropna()"
      ],
      "metadata": {
        "id": "vXWad_xvU9OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Org dataset amount:{len(tds)}')\n",
        "print(f'Org dataset amount:{len(ev_ds)}')\n",
        "print(f'Org dataset amount:{len(f_ev_ds)}')\n",
        "\n",
        "print(f'Dropped Dataset amount:{len(tds_noNAN)}')\n",
        "print(f'Dropped Dataset amount:{len(ev_ds_noNAN)}')\n",
        "print(f'Dropped Dataset amount:{len(f_ev_ds_noNAN)}')"
      ],
      "metadata": {
        "id": "o2L9iJoAU529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define what is congestion and what is not.\n",
        "# congestion_flow = tds[tds['FLOW'] < 100]\n",
        "# congestion_speed = tds[tds['SPEED_MS_AVG'] < 10]\n",
        "\n",
        "ev_ds_noNAN['datetime'] = pd.to_datetime(ev_ds_noNAN['Date'].astype(str) + ' ' + ev_ds_noNAN['Time'])\n",
        "\n",
        "ev_ds_offpeak = ev_ds_noNAN[(ev_ds_noNAN['datetime'].dt.time >= pd.to_datetime(\"04:00:00\").time()) &\n",
        "                            (ev_ds_noNAN['datetime'].dt.time <= pd.to_datetime(\"06:00:00\").time())]\n",
        "ev_ds_onpeak = ev_ds_noNAN[(ev_ds_noNAN['datetime'].dt.time >= pd.to_datetime(\"07:30:00\").time()) &\n",
        "                           (ev_ds_noNAN['datetime'].dt.time <= pd.to_datetime(\"08:30:00\").time())]\n",
        "\n",
        "ev_ds_offpeak = ev_ds_offpeak[(ev_ds_offpeak['FLOW'] > 0) &\n",
        "                              (ev_ds_offpeak['SPEED_MS_AVG'].between(5, 50))]\n",
        "ev_ds_onpeak = ev_ds_onpeak[(ev_ds_onpeak['FLOW'] > 0) &\n",
        "                            (ev_ds_onpeak['SPEED_MS_AVG'].between(5, 50))]\n"
      ],
      "metadata": {
        "id": "oezW0AffMMBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the date and time columns int o just one \"datetime\" coluinm\n",
        "tds_noNAN['datetime'] = pd.to_datetime(tds_noNAN['Date'].astype(str) + ' ' + tds_noNAN['Time'])\n",
        "\n",
        "# filtering the dataset to only include the data within hours 04:00 and 06:00, this to get an accurate description of free flow speed.\n",
        "tds_offpeak = tds_noNAN[(tds_noNAN['datetime'].dt.time >= pd.to_datetime(\"04:00:00\").time()) &\n",
        "                (tds_noNAN['datetime'].dt.time <= pd.to_datetime(\"06:00:00\").time())]\n",
        "\n",
        "tds_onpeak= tds_noNAN[(tds_noNAN['datetime'].dt.time >= pd.to_datetime(\"07:30:00\").time()) &\n",
        "                  (tds_noNAN['datetime'].dt.time <= pd.to_datetime(\"08:30:00\").time())]\n",
        "#Ensuring we only get correctly read speeds. E.g Removing any negative speeds and random slow drivers etc\n",
        "#that do not actually depict the actual free flowspeed e.g people speeding and drinving super slow...\n",
        "tds_offpeak = tds_offpeak[(tds_offpeak['FLOW'] > 0) &\n",
        "                        (tds_offpeak['SPEED_MS_AVG'].between(5, 50))]\n",
        "tds_onpeak = tds_onpeak[(tds_onpeak['FLOW'] > 0) &\n",
        "                        (tds_onpeak['SPEED_MS_AVG'].between(5, 50))]\n",
        "# Calculating the free flow speed\n",
        "# only take the observations in the 85th quantile [m/s]\n",
        "ffs_ms = (tds_offpeak['SPEED_MS_AVG'].quantile(0.85))\n",
        "ffs_ms_2 = (tds_onpeak['SPEED_MS_AVG'].quantile(0.85))\n",
        "# convert to km/h\n",
        "ffs_kmh = (ffs_ms * 3.6)\n",
        "ffs_kmh_2 =(ffs_ms_2 * 3.6)\n",
        "print(f\"Free Flow Speed [85th percentile]: \\n Free flow speed: {ffs_ms_2 } [m/s] \\n Free Flow Speed {ffs_kmh_2} [km/h]\")\n",
        "print(f\"Free-flow speed (85th percentile): \\n Free flow speed: {ffs_ms} [m/s] \\n Free flow speed {ffs_kmh} [km/h]\")"
      ],
      "metadata": {
        "id": "Ou3DA0M5VJKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate FFS (85th percentile)\n",
        "ffs_ms_2 = tds_onpeak['SPEED_MS_AVG'].quantile(0.85)\n",
        "ffs_kmh_2 = ffs_ms_2 * 3.6\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(tds_onpeak['SPEED_MS_AVG']*3.6, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.axvline(ffs_kmh_2, color='red', linestyle='--', linewidth=2, label=f'85th percentile (FFS = {ffs_kmh_2:.2f} km/h)')\n",
        "plt.xlabel(\"Speed (km/h)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Speeds During On-Peak (07:30–08:30)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ak48AujNQBWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate FFS (85th percentile)\n",
        "ffs_ms = tds_offpeak['SPEED_MS_AVG'].quantile(0.85)\n",
        "ffs_kmh = ffs_ms * 3.6\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(tds_offpeak['SPEED_MS_AVG']*3.6, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(ffs_kmh, color='red', linestyle='--', linewidth=2, label=f'85th percentile (FFS = {ffs_kmh:.2f} km/h)')\n",
        "\n",
        "plt.xlabel(\"Speed (km/h)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Speeds During Off-Peak (04:00–06:00)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uBlKaW0X-2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Congestion Threshold!\n",
        "cong_th = ffs_kmh * 0.8 # why 70% of the ffs?\n",
        "cong_th_2 = ffs_kmh_2 * 0.8\n",
        "\n",
        "print(f'There is congestion at the avg. speed of: {cong_th} [km/h]')\n",
        "print(f'There is congestion at the avg. speed of : {cong_th_2} [km/h]')"
      ],
      "metadata": {
        "id": "R5ParPnNbdFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tds_noNAN['pred15'] = tds_noNAN['SPEED_MS_AVG'].shift(-1).rolling(15).mean()\n",
        "# # congestion if 15 min av speed is less than cong_th (rn 60 km/h)\n",
        "# tds_noNAN['congestion'] = (tds_noNAN['pred15']*3.6 <= cong_th).astype(int)\n",
        "\n",
        "tds_noNAN['pred15'] = tds_noNAN['SPEED_MS_AVG'].shift(-1).rolling(15).mean()\n",
        "tds_noNAN['congestion'] = (tds_noNAN['pred15']*3.6 <= cong_th_2).astype(int)"
      ],
      "metadata": {
        "id": "qScNu1yw0KIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_features_targets(df, horizon=15):\n",
        "\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # --- Target: average speed in the next <horizon> minutes (leakage-safe) ---\n",
        "    df[f'target_speed_{horizon}m'] = (\n",
        "        df['SPEED_MS_AVG'].shift(-horizon).rolling(horizon).mean() * 3.6\n",
        "    )\n",
        "\n",
        "    # --- Feature engineering: past 15-min rolling stats ---\n",
        "    df['past15_flow_mean']  = df['FLOW'].rolling(15).mean()\n",
        "    df['past15_flow_std']   = df['FLOW'].rolling(15).std()\n",
        "    df['past15_flow_min']   = df['FLOW'].rolling(15).min()\n",
        "    df['past15_flow_max']   = df['FLOW'].rolling(15).max()\n",
        "    df['past15_flow_slope'] = df['FLOW'] - df['FLOW'].shift(15)\n",
        "\n",
        "    df['past15_speed_mean']  = df['SPEED_MS_AVG'].rolling(15).mean()\n",
        "    df['past15_speed_std']   = df['SPEED_MS_AVG'].rolling(15).std()\n",
        "    df['past15_speed_min']   = df['SPEED_MS_AVG'].rolling(15).min()\n",
        "    df['past15_speed_max']   = df['SPEED_MS_AVG'].rolling(15).max()\n",
        "    df['past15_speed_slope'] = df['SPEED_MS_AVG'] - df['SPEED_MS_AVG'].shift(15)\n",
        "\n",
        "    # --- (Optional) Time-of-day cyclical features ---\n",
        "    df[\"minute_of_day\"] = df[\"datetime\"].dt.hour * 60 + df[\"datetime\"].dt.minute\n",
        "    df[\"tod_sin\"] = np.sin(2 * np.pi * df[\"minute_of_day\"] / 1440)\n",
        "    df[\"tod_cos\"] = np.cos(2 * np.pi * df[\"minute_of_day\"] / 1440)\n",
        "\n",
        "    features = [\n",
        "        'past15_flow_mean','past15_flow_std','past15_flow_min','past15_flow_max','past15_flow_slope',\n",
        "        'past15_speed_mean','past15_speed_std','past15_speed_min','past15_speed_max','past15_speed_slope',\n",
        "        'tod_sin','tod_cos'\n",
        "    ]\n",
        "\n",
        "    target = f'target_speed_{horizon}m'\n",
        "\n",
        "    # Drop rows that can't compute full windows\n",
        "    df = df.dropna(subset=features + [target])\n",
        "\n",
        "    return df, features, target\n"
      ],
      "metadata": {
        "id": "zC_0zwpGEDa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create training data (for 2021)\n",
        "tds_fixad, features, target = make_features_targets(tds_noNAN, horizon=15)\n",
        "# 2. Create evaluation data (for 2022)\n",
        "ev_fixad, _, _ = make_features_targets(ev_ds_noNAN, horizon=15)\n"
      ],
      "metadata": {
        "id": "5pCZiowXHPdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train / test sets\n",
        "X = tds_fixad[features].values\n",
        "y = tds_fixad[target].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "#Scaling the datat.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wsZWjSzpKOQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "qjbV9vQfLA_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#perform linear regression model.\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "mape_lr = mean_absolute_percentage_error(y_test, y_pred_lr)"
      ],
      "metadata": {
        "id": "7ftEGsokKqdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Random forrest model\n",
        "# random_f = RandomForestRegressor(n_estimators = 100, max_depth = 3, random_state = 42, n_jobs=-1)\n",
        "# random_f.fit(X_train, y_train)\n",
        "# y_pred_rf = random_f.predict(X_test)"
      ],
      "metadata": {
        "id": "ee0TeBdyLNu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "mae_xgb= mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "rmse_xgb = np.sqrt(mse_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)\n"
      ],
      "metadata": {
        "id": "uNGiydnpNax4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Regression\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lr}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_lr}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_lr}\")\n",
        "print(f\"R2-score {r2_lr}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape_lr}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape_xgb}\")\n",
        "print(f\"R2-score {r2_xgb}\")\n"
      ],
      "metadata": {
        "id": "OaC7hdtBOxA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}